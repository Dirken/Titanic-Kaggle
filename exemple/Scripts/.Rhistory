dataMar <- read.csv("data_Mar_64.txt",header = FALSE)
dataSha <- read.csv("data_Sha_64.txt",header = FALSE)
dataTex <- read.csv("data_Tex_64.txt",header = FALSE)
#### TRATAMIENTO COLUMNAS #####
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataTex <- dataTex[,-1]
dataSha <- dataSha[,-1]
dataMar <- dataMar[,-1]
##### TRATAMIENTO FILAS #####
listTmp <- rep(length(dataTex))
for (i in seq(2,length(dataTex), by=1)) {
listTmp[i-1] = mean(dataTex[1:15,i])
}
dataTex <- rbind(dataTex,listTmp)
##### TRATAMIENTO MAGNITUD VALORES #####
dataTmp <- cbind(dataMar,dataSha,dataTex)
dataTmp <- scale(dataTmp)
##### TO MATRIX #####
dataC = matrix(ncol=length(dataTmp[1,]),nrow=length(dataTmp[,1]))
for (i in seq(1,length(dataTmp[1,]), by=1)) {
dataC[,i] = dataTmp[,i]
}
##### CLUSTERING #####
K <- 10
kmeans <- cclust (dataC,K,iter.max=100,method="kmeans",dist="euclidean")
data <- cbind(kmeans$cluster,dataC)
dataTex <- cbind(kmeans$cluster,dataTex)
dataSha <- cbind(kmeans$cluster,dataSha)
dataMar <- cbind(kmeans$cluster,dataMar)
View(dataTex)
dataTex <- scale(dataTex)
View(dataTex)
library(MASS)
library(cclust)
library(stringr)
set.seed(4)
#### LECTURA FICHEROS #####
dataMar <- read.csv("data_Mar_64.txt",header = FALSE)
dataSha <- read.csv("data_Sha_64.txt",header = FALSE)
dataTex <- read.csv("data_Tex_64.txt",header = FALSE)
#### TRATAMIENTO COLUMNAS #####
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataTex <- dataTex[,-1]
dataSha <- dataSha[,-1]
dataMar <- dataMar[,-1]
##### TRATAMIENTO FILAS #####
listTmp <- rep(length(dataTex))
for (i in seq(2,length(dataTex), by=1)) {
listTmp[i-1] = mean(dataTex[1:15,i])
}
dataTex <- rbind(dataTex,listTmp)
##### TRATAMIENTO MAGNITUD VALORES #####
dataTmp <- cbind(dataMar,dataSha,dataTex)
dataTmp <- scale(dataTmp)
##### TO MATRIX #####
dataC = matrix(ncol=length(dataTmp[1,]),nrow=length(dataTmp[,1]))
for (i in seq(1,length(dataTmp[1,]), by=1)) {
dataC[,i] = dataTmp[,i]
}
##### CLUSTERING #####
K <- 10
kmeans <- cclust (dataC,K,iter.max=100,method="kmeans",dist="euclidean")
data <- cbind(kmeans$cluster,dataC)
dataTex <- cbind(kmeans$cluster,dataTex)
dataSha <- cbind(kmeans$cluster,dataSha)
dataMar <- cbind(kmeans$cluster,dataMar)
View(dataTex)
dataTmp <- scale(dataTex[,-1])
dataTex <- cbind(as.factor(dataMar[,1]),dataTmp)
View(dataTex)
dataTmp <- scale(dataTex[,-1])
dataTex <- cbind(as.factor(dataTex[,1]),dataTmp)
dataTmp <- scale(dataSha[,-1])
dataSha <- cbind(as.factor(dataSha[,1]),dataTmp)
dataTmp <- scale(dataMar[,-1])
dataMar <- cbind(as.factor(dataMar[,1]),dataTmp)
data <- as.data.frame(data)
dataTex <- as.data.frame(dataTex)
dataSha <- as.data.frame(dataSha)
dataMar <- as.data.frame(dataMar)
names(data)[1]<-paste("Target")
names(dataTex)[1]<-paste("Target")
names(dataSha)[1]<-paste("Target")
names(dataMar)[1]<-paste("Target")
data$Target <- as.factor(data$Target)
dataTex$Target <- as.factor(dataTex$Target)
dataSha$Target <- as.factor(dataSha$Target)
dataMar$Target <- as.factor(dataMar$Target)
##### REDUCCION VARIABLES #####
library(FSelector)
library(CORElearn)
library(ipred)
subsetTex.CFS <- cfs (Target~., dataTex)
dataTex.CFS <- cbind(dataTex[1],dataTex[subsetTex.CFS])
subsetSha.CFS <- cfs (Target~., dataSha)
dataSha.CFS <- cbind(dataSha[1],dataSha[subsetSha.CFS])
subsetMar.CFS <- cfs (Target~., dataMar)
dataMar.CFS <- cbind(dataMar[1],dataMar[subsetMar.CFS])
weights.randomForest <- random.forest.importance (Target~., dataTex.CFS, importance.type = 1)
dataTex.CRF <- cbind(dataTex[1],dataTex[cutoff.k(weights.randomForest, 20)])
weights.randomForest <- random.forest.importance (Target~., dataSha.CFS, importance.type = 1)
dataSha.CRF <- cbind(dataSha[1],dataSha[cutoff.k(weights.randomForest, 20)])
weights.randomForest <- random.forest.importance (Target~., dataMar.CFS, importance.type = 1)
dataMar.CRF <- cbind(dataMar[1],dataMar[cutoff.k(weights.randomForest, 20)])
##### TESTEO DE LAS VARIABLES #####
data.CRF <- cbind(dataTex.CRF[,1])
names(data.CRF)[1]<-paste("Target")
data.CRF$Target <- as.factor(data.CRF$Target)
K <- 10
View(data.CRF)
data.CRF <- as.factor(data.CRF)
K <- 10
TIMES <- 10
#->INI
dataTest <- dataTex.CRF
mycontrol.10 <- control.errorest (k = K, strat = TRUE, random = FALSE, predictions = TRUE)
mypredict <- function(object, newdata)
predict(object, newdata = newdata)$class
evaluator.accuracy <- function (subset)
{
cat(length(subset), subset)
print(1 - mean(replicate(TIMES,errorest (as.simple.formula(subset, "Target"),
data=dataTest,
model=mymethod,
estimator = "cv",
predict = mypredict,
est.para=mycontrol.10)$error)))
}
mymethod <- lda
subset <- forward.search(names(dataTest)[-1], evaluator.accuracy)
f.lda2 <- as.simple.formula(subset, "Target")
print(f.lda2)
data.CRF <- cbind(data.CRF,dataTest[,subset])
dataTest <- dataSha.CRF
mycontrol.10 <- control.errorest (k = K, strat = TRUE, random = FALSE, predictions = TRUE)
mypredict <- function(object, newdata)
predict(object, newdata = newdata)$class
evaluator.accuracy <- function (subset)
{
cat(length(subset), subset)
print(1 - mean(replicate(TIMES,errorest (as.simple.formula(subset, "Target"),
data=dataTest,
model=mymethod,
estimator = "cv",
predict = mypredict,
est.para=mycontrol.10)$error)))
}
mymethod <- lda
subset <- forward.search(names(dataTest)[-1], evaluator.accuracy)
f.lda2 <- as.simple.formula(subset, "Target")
print(f.lda2)
data.CRF <- cbind(data.CRF,dataTest[,subset])
dataTest <- dataMar.CRF
mycontrol.10 <- control.errorest (k = K, strat = TRUE, random = FALSE, predictions = TRUE)
mypredict <- function(object, newdata)
predict(object, newdata = newdata)$class
evaluator.accuracy <- function (subset)
{
cat(length(subset), subset)
print(1 - mean(replicate(TIMES,errorest (as.simple.formula(subset, "Target"),
data=dataTest,
model=mymethod,
estimator = "cv",
predict = mypredict,
est.para=mycontrol.10)$error)))
}
mymethod <- lda
subset <- forward.search(names(dataTest)[-1], evaluator.accuracy)
f.lda2 <- as.simple.formula(subset, "Target")
print(f.lda2)
data.CRF <- cbind(data.CRF,dataTest[,subset])
dataTest <- data.CRF
mycontrol.10 <- control.errorest (k = K, strat = TRUE, random = FALSE, predictions = TRUE)
mypredict <- function(object, newdata)
predict(object, newdata = newdata)$class
evaluator.accuracy <- function (subset)
{
cat(length(subset), subset)
print(1 - mean(replicate(TIMES,errorest (as.simple.formula(subset, "Target"),
data=dataTest,
model=mymethod,
estimator = "cv",
predict = mypredict,
est.para=mycontrol.10)$error)))
}
mymethod <- lda
subset <- forward.search(names(dataTest)[-1], evaluator.accuracy)
data.CRF$Target <- as.factor(data.CRF$Target)
data.CRF$Target <- as.factor(data.CRF$Target)
View(data.CRF)
names(data.CRF)[1]<-paste("Target")
data.CRF$Target <- as.factor(data.CRF$Target)
K <- 10
TIMES <- 10
#->INI
dataTest <- data.CRF
mycontrol.10 <- control.errorest (k = K, strat = TRUE, random = FALSE, predictions = TRUE)
mypredict <- function(object, newdata)
predict(object, newdata = newdata)$class
evaluator.accuracy <- function (subset)
{
cat(length(subset), subset)
print(1 - mean(replicate(TIMES,errorest (as.simple.formula(subset, "Target"),
data=dataTest,
model=mymethod,
estimator = "cv",
predict = mypredict,
est.para=mycontrol.10)$error)))
}
mymethod <- lda
subset <- forward.search(names(dataTest)[-1], evaluator.accuracy)
f.lda2 <- as.simple.formula(subset, "Target")
print(f.lda2)
library(MASS)
library(cclust)
library(stringr)
set.seed(4)
#### LECTURA FICHEROS #####
dataMar <- read.csv("data_Mar_64.txt",header = FALSE)
dataSha <- read.csv("data_Sha_64.txt",header = FALSE)
dataTex <- read.csv("data_Tex_64.txt",header = FALSE)
#### TRATAMIENTO COLUMNAS #####
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataTex <- dataTex[,-1]
dataSha <- dataSha[,-1]
##### TRATAMIENTO FILAS #####
listTmp <- rep(length(dataTex))
for (i in seq(2,length(dataTex), by=1)) {
listTmp[i-1] = mean(dataTex[1:15,i])
}
dataTex <- rbind(dataTex,listTmp)
##### RENOMBRAR FILAS #####
dataMar$V1<-as.character(lapply(dataMar$V1, function(x) {
pos<-str_locate(x,' ')
if (!is.na(pos[1,1]))
(substr(x,0,pos))
else x
}))
##### TRATAMIENTO MAGNITUD VALORES #####
dataTmp <- cbind(dataMar[,-1],dataSha,dataTex)
dataTmp <- scale(dataTmp)
data <- cbind(as.factor(dataMar[,1]),dataTmp)
##### RENOMBRAR COLUMNAS #####
data <- as.data.frame(data)
names(data)[1]<-paste("Target")
names(data)[2:length(data[1,])] <- paste("V", 2:length(data[1,]), sep = "")
data$Target<-as.factor(data$Target)
#### LEARN & TEST ####
library(FSelector)
row <- length(data[,1])
learn <- sample(1:row, row/2)
learn
table(data$Target[learn])
data.learn <- data[learn, ]
data.test <- data[-learn, ]
##### REDUCCION VARIABLES #####
subset.CFS <- cfs (Target~., data.learn)
summary(data.learn)
summary(data.learn$Target)
subset.CFS <- cfs (Target~., data.learn)
View(data.learn)
sumary(data)
summary(data)
subset.CFS <- cfs (Target~., data.learn)
data.CFS <- cbind(data.learn[1],data.learn[subset.CFS])
# Correlation filter + Random Forest (20 variables)
weights.randomForest <- random.forest.importance (Target~., data.CFS, importance.type = 1)
data.CRF <- cbind(data.learn[1],data.learn[cutoff.k(weights.randomForest, 20)])
dif(data$Target)
diff(data$Target)
View(data)
diff(data[,1])
?diff
?count
Summary(data)
summary(data)
summary(data&Target)
summary(data$Target)
max(data$Target)
View(data.CRF)
max(data.CRF$Target)
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1)/10, data = data, subset=learn, CV=TRUE)
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)/10, data = data, subset=learn, CV=TRUE)
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)/32, data = data, subset=learn, CV=TRUE)
library(MASS)
library(cclust)
library(stringr)
set.seed(4)
#### LECTURA FICHEROS #####
dataMar <- read.csv("data_Mar_64.txt",header = FALSE)
dataSha <- read.csv("data_Sha_64.txt",header = FALSE)
dataTex <- read.csv("data_Tex_64.txt",header = FALSE)
#### TRATAMIENTO COLUMNAS #####
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataTex <- dataTex[,-1]
dataSha <- dataSha[,-1]
dataMar <- dataMar[,-1]
##### TRATAMIENTO FILAS #####
listTmp <- rep(length(dataTex))
for (i in seq(2,length(dataTex), by=1)) {
listTmp[i-1] = mean(dataTex[1:15,i])
}
dataTex <- rbind(dataTex,listTmp)
##### TRATAMIENTO MAGNITUD VALORES #####
dataTmp <- cbind(dataMar,dataSha,dataTex)
dataTmp <- scale(dataTmp)
##### TO MATRIX #####
dataC = matrix(ncol=length(dataTmp[1,]),nrow=length(dataTmp[,1]))
for (i in seq(1,length(dataTmp[1,]), by=1)) {
dataC[,i] = dataTmp[,i]
}
##### CLUSTERING #####
K <- 10
kmeans <- cclust (dataC,K,iter.max=100,method="kmeans",dist="euclidean")
data <- cbind(kmeans$cluster,dataC)
data <- as.data.frame(data)
names(data)[1]<-paste("Target")
data$Target <- factor(data$Target)
##### VISUALIDAR DATOS - LDA #####
data.lda <- lda(Target ~ ., data, prior = seq(1,1,length.out=K)/K)
data.lda
plot(data.lda, col=as.numeric(data$Target))
library(FSelector)
row <- length(data[,1])
learn <- sample(1:row, row/2)
learn
table(data$Target[learn])
data.learn <- data[learn, ]
data.test <- data[-learn, ]
##### REDUCCION VARIABLES #####
subset.CFS <- cfs (Target~., data.learn)
data.CFS <- cbind(data.learn[1],data.learn[subset.CFS])
# Correlation filter + Random Forest (20 variables)
weights.randomForest <- random.forest.importance (Target~., data.CFS, importance.type = 1)
data.CRF <- cbind(data.learn[1],data.learn[cutoff.k(weights.randomForest, 20)])
#### CLASIFICATION ####
library(klaR)
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1)/10, data = data, subset=CRF, CV=TRUE)
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1)/10, data = data, subset=data.CRF, CV=TRUE)
View(data.CRF)
?lda
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1)/10, data = data.CRF, subset=learn, CV=TRUE)
summary(data.lda.cv$class)
#### TESTING ####
tab <- table(data$Target[learn], data.lda.cv$class)
tab <- table(data.CRF$Target[learn], data.lda.cv$class)
length(data.CRF)
library(MASS)
library(cclust)
library(stringr)
set.seed(4)
#### LECTURA FICHEROS #####
dataMar <- read.csv("data_Mar_64.txt",header = FALSE)
dataSha <- read.csv("data_Sha_64.txt",header = FALSE)
dataTex <- read.csv("data_Tex_64.txt",header = FALSE)
#### TRATAMIENTO COLUMNAS #####
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataTex <- dataTex[,-1]
dataSha <- dataSha[,-1]
dataMar <- dataMar[,-1]
##### TRATAMIENTO FILAS #####
listTmp <- rep(length(dataTex))
for (i in seq(2,length(dataTex), by=1)) {
listTmp[i-1] = mean(dataTex[1:15,i])
}
dataTex <- rbind(dataTex,listTmp)
##### TRATAMIENTO MAGNITUD VALORES #####
dataTmp <- cbind(dataMar,dataSha,dataTex)
dataTmp <- scale(dataTmp)
##### TO MATRIX #####
dataC = matrix(ncol=length(dataTmp[1,]),nrow=length(dataTmp[,1]))
for (i in seq(1,length(dataTmp[1,]), by=1)) {
dataC[,i] = dataTmp[,i]
}
##### CLUSTERING #####
K <- 10
kmeans <- cclust (dataC,K,iter.max=100,method="kmeans",dist="euclidean")
data <- cbind(kmeans$cluster,dataC)
data <- as.data.frame(data)
names(data)[1]<-paste("Target")
data$Target <- factor(data$Target)
##### VISUALIDAR DATOS - LDA #####
data.lda <- lda(Target ~ ., data, prior = seq(1,1,length.out=K)/K)
data.lda
plot(data.lda, col=as.numeric(data$Target))
library(FSelector)
subset.CFS <- cfs (Target~., data.learn)
subset.CFS <- cfs (Target~., data)
data.CFS <- cbind(data[1],data[subset.CFS])
# Correlation filter + Random Forest (20 variables)
weights.randomForest <- random.forest.importance (Target~., data.CFS, importance.type = 1)
data.CRF <- cbind(data[1],data[cutoff.k(weights.randomForest, 20)])
#### LEARN & TEST ####
row <- length(data.CRF[,1])
learn <- sample(1:row, row/2)
learn
table(data.CRF$Target[learn])
data.learn <- data.CRF[learn, ]
data.test <- data.CRF[-learn, ]
#### CLASIFICATION ####
library(klaR)
?lda
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1)/10, data = data.CRF, subset=learn, CV=TRUE)
summary(data.lda.cv$class)
#### TESTING ####
length(data.CRF)
tab <- table(data.CRF$Target[learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
tab <- table(data$Target[learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
tab <- table(data$Target[learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
tab <- table(data.test$Target[learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
tab <- table(data$Target[-learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
tab <- table(data.CRF$Target[-learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
tab <- table(data.CRF$Target[learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
library(MASS)
library(cclust)
library(stringr)
set.seed(4)
#### LECTURA FICHEROS #####
dataMar <- read.csv("data_Mar_64.txt",header = FALSE)
dataSha <- read.csv("data_Sha_64.txt",header = FALSE)
dataTex <- read.csv("data_Tex_64.txt",header = FALSE)
#### TRATAMIENTO COLUMNAS #####
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataMar <- dataMar[,-17]
dataTex <- dataTex[,-22]
dataTex <- dataTex[,-61]
dataTex <- dataTex[,-1]
dataSha <- dataSha[,-1]
dataMar <- dataMar[,-1]
##### TRATAMIENTO FILAS #####
listTmp <- rep(length(dataTex))
for (i in seq(2,length(dataTex), by=1)) {
listTmp[i-1] = mean(dataTex[1:15,i])
}
dataTex <- rbind(dataTex,listTmp)
##### TRATAMIENTO MAGNITUD VALORES #####
dataTmp <- cbind(dataMar,dataSha,dataTex)
dataTmp <- scale(dataTmp)
##### TO MATRIX #####
dataC = matrix(ncol=length(dataTmp[1,]),nrow=length(dataTmp[,1]))
for (i in seq(1,length(dataTmp[1,]), by=1)) {
dataC[,i] = dataTmp[,i]
}
##### CLUSTERING #####
K <- 10
kmeans <- cclust (dataC,K,iter.max=100,method="kmeans",dist="euclidean")
data <- cbind(kmeans$cluster,dataC)
data <- as.data.frame(data)
names(data)[1]<-paste("Target")
data$Target <- factor(data$Target)
##### VISUALIDAR DATOS - LDA #####
data.lda <- lda(Target ~ ., data, prior = seq(1,1,length.out=K)/K)
data.lda
plot(data.lda, col=as.numeric(data$Target))
library(FSelector)
library(CORElearn)
library(ipred)
##### REDUCCION VARIABLES #####
subset.CFS <- cfs (Target~., data)
data.CFS <- cbind(data[1],data[subset.CFS])
weights.randomForest <- random.forest.importance (Target~., data.CFS, importance.type = 1)
data.CRF <- cbind(data[1],data[cutoff.k(weights.randomForest, 20)])
library(FSelector)
row <- length(data.CRF[,1])
learn <- sample(1:row, row/2)
data.learn <- data.CRF[learn, ]
data.test <- data.CRF[-learn, ]
#### LEARNING ####
data.lda.cv <- lda(Target ~ ., prior = c(1,1,1,1,1,1,1,1,1,1)/10, data = data.CRF, subset=learn, CV=TRUE)
summary(data.lda.cv$class)
tab <- table(data.CRF$Target[learn], data.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
#### TESTING ####
pred <- predict(data.lda.cv,data.test[,2:21])
typeof(data.test)
data.test <- as.dataframe(data.test)
data.test <- dataframe(data.test)
pred <- predict(data.lda.cv,data.test[,2:21])
